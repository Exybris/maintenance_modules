"""
explore.py - Exploration, d√©tection d'√©mergence et d'anomalie FPS
Version exhaustive conforme √† la feuille de route FPS V1.3
---------------------------------------------------------------
NOTE FPS ‚Äì Plasticit√© m√©thodologique :
Ce module capture les ph√©nom√®nes non anticip√©s ou √©mergents.
Il doit rester ouvert, extensible, et permettre √† chaque contributeur
d'ajouter ses propres d√©tecteurs ou analyses.
---------------------------------------------------------------

Ce module r√©v√®le l'invisible dans la dynamique FPS :
- Anomalies persistantes et √©v√©nements chaotiques
- Bifurcations spiral√©es et transitions de phase
- √âmergences harmoniques et nouveaux motifs
- Patterns fractals et auto-similarit√©
- Cycles attracteurs dans l'espace de phase

Toute √©mergence d√©tect√©e est logu√©e, tra√ßable (avec seed/config),
et sujette √† reproductibilit√©.

(c) 2025 Gepetto & Andr√©a Gadal & Claude üåÄ
"""

import numpy as np
import json
import csv
import os
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any, Union
from scipy import signal, spatial
from scipy.stats import entropy
import warnings
from collections import defaultdict

# Imports pour coh√©rence avec les autres modules
try:
    import h5py
    HDF5_AVAILABLE = True
except ImportError:
    HDF5_AVAILABLE = False
    warnings.warn("h5py non disponible - lecture HDF5 d√©sactiv√©e")


# ============== ORCHESTRATION PRINCIPALE ==============

def run_exploration(run_data_path: str, output_dir: str, 
                   config: Optional[Dict] = None) -> Dict[str, Any]:
    """
    Orchestration compl√®te de l'exploration post-run.
    
    1. Charge les logs du run (CSV/HDF5)
    2. Lance tous les d√©tecteurs sur les m√©triques d√©finies
    3. Agr√®ge les √©v√©nements et les exporte
    4. G√©n√®re un rapport Markdown d√©taill√©
    
    Args:
        run_data_path: chemin vers les logs CSV ou HDF5
        output_dir: dossier de sortie pour les r√©sultats
        config: configuration (optionnelle, sinon charg√©e depuis config.json)
    
    Returns:
        Dict avec tous les r√©sultats d'exploration
    """
    print(f"\n=== Exploration FPS : {os.path.basename(run_data_path)} ===")
    
    # Cr√©er le dossier de sortie
    os.makedirs(output_dir, exist_ok=True)
    
    # Charger la configuration si non fournie
    if config is None:
        config = load_config_for_exploration()
    
    exploration_config = config.get('exploration', {})
    
    # Charger les donn√©es du run
    print("üìä Chargement des donn√©es...")
    data = load_run_data(run_data_path)
    
    if not data:
        print("‚ùå Impossible de charger les donn√©es")
        return {'status': 'error', 'message': 'Donn√©es non charg√©es'}
    
    # Extraire le run_id du nom de fichier
    run_id = extract_run_id(run_data_path)
    
    # Collecter tous les √©v√©nements
    all_events = []
    
    # 1. D√©tection d'anomalies
    if exploration_config.get('detect_anomalies', True):
        print("\nüîç D√©tection d'anomalies...")
        anomalies = detect_anomalies(
            data,
            exploration_config.get('metrics', ['S(t)', 'C(t)', 'effort(t)']),
            exploration_config.get('anomaly_threshold', 3.0),
            exploration_config.get('min_duration', 3)
        )
        all_events.extend(anomalies)
        print(f"  ‚Üí {len(anomalies)} anomalies d√©tect√©es")
    
    # 2. D√©tection de bifurcations spiral√©es
    print("\nüåÄ D√©tection de bifurcations...")
    bifurcations = detect_spiral_bifurcations(
        data,
        phase_metric='C(t)',
        threshold=np.pi
    )
    all_events.extend(bifurcations)
    print(f"  ‚Üí {len(bifurcations)} bifurcations d√©tect√©es")
    
    # 3. D√©tection d'√©mergences harmoniques
    if exploration_config.get('detect_harmonics', True):
        print("\nüéµ D√©tection d'√©mergences harmoniques...")
        harmonics = detect_harmonic_emergence(
            data,
            signal_metric='S(t)',
            n_harmonics=5,
            window=100,
            step=10
        )
        all_events.extend(harmonics)
        print(f"  ‚Üí {len(harmonics)} √©mergences harmoniques")
    
    # 4. Exploration de l'espace de phase
    print("\nüìà Exploration de l'espace de phase...")
    phase_events = explore_phase_space(
        data,
        metric='S(t)',
        window=50,
        min_diagonal_length=5
    )
    all_events.extend(phase_events)
    print(f"  ‚Üí {len(phase_events)} patterns dans l'espace de phase")
    
    # 5. D√©tection de motifs fractals
    if exploration_config.get('detect_fractal_patterns', True):
        print("\nüåø D√©tection de motifs fractals...")
        fractal_events = detect_fractal_patterns(
            data,
            metrics=exploration_config.get('metrics', ['S(t)', 'C(t)', 'effort(t)']),
            window_sizes=exploration_config.get('window_sizes', [1, 10, 100]),
            threshold=exploration_config.get('fractal_threshold', 0.8)
        )
        all_events.extend(fractal_events)
        print(f"  ‚Üí {len(fractal_events)} motifs fractals d√©tect√©s")
        
        # Logger les √©v√©nements fractals s√©par√©ment
        if fractal_events:
            fractal_log_path = os.path.join(output_dir, f"fractal_events_{run_id}.csv")
            log_fractal_events(fractal_events, fractal_log_path)
    
    # 6. Exporter tous les √©v√©nements
    events_path = os.path.join(output_dir, f"emergence_events_{run_id}.csv")
    log_events(all_events, events_path)
    print(f"\nüíæ √âv√©nements export√©s : {events_path}")
    
    # 7. G√©n√©rer le rapport
    report_path = os.path.join(output_dir, f"exploration_report_{run_id}.md")
    generate_report(all_events, report_path, run_id, config)
    print(f"üìÑ Rapport g√©n√©r√© : {report_path}")
    
    # R√©sum√© des r√©sultats
    results = {
        'status': 'success',
        'run_id': run_id,
        'total_events': len(all_events),
        'events_by_type': count_events_by_type(all_events),
        'events': all_events,
        'paths': {
            'events': events_path,
            'report': report_path,
            'fractal_events': os.path.join(output_dir, f"fractal_events_{run_id}.csv") if fractal_events else None
        }
    }
    
    print(f"\n‚úÖ Exploration termin√©e : {len(all_events)} √©v√©nements d√©tect√©s")
    
    return results


# ============== CHARGEMENT DES DONN√âES ==============

def load_run_data(data_path: str) -> Dict[str, np.ndarray]:
    """
    Charge les donn√©es depuis CSV ou HDF5.
    """
    if data_path.endswith('.csv'):
        return load_csv_data(data_path)
    elif data_path.endswith('.h5') or data_path.endswith('.hdf5'):
        if HDF5_AVAILABLE:
            return load_hdf5_data(data_path)
        else:
            warnings.warn("HDF5 non disponible - impossible de lire le fichier")
            return {}
    else:
        warnings.warn(f"Format non reconnu : {data_path}")
        return {}


def load_csv_data(csv_path: str) -> Dict[str, np.ndarray]:
    """
    Charge les donn√©es depuis un fichier CSV.
    """
    data = defaultdict(list)
    
    try:
        with open(csv_path, 'r') as f:
            reader = csv.DictReader(f)
            for row in reader:
                for key, value in row.items():
                    try:
                        # Convertir en float si possible
                        if value and value.lower() not in ['stable', 'transitoire', 'chronique']:
                            data[key].append(float(value))
                        else:
                            data[key].append(value)
                    except ValueError:
                        data[key].append(value)
        
        # Convertir en arrays numpy
        for key in data:
            if data[key] and isinstance(data[key][0], (int, float)):
                data[key] = np.array(data[key])
        
        return dict(data)
        
    except Exception as e:
        warnings.warn(f"Erreur chargement CSV : {e}")
        return {}


def load_hdf5_data(hdf5_path: str) -> Dict[str, np.ndarray]:
    """
    Charge les donn√©es depuis un fichier HDF5.
    """
    data = {}
    
    try:
        with h5py.File(hdf5_path, 'r') as f:
            # Parcourir tous les groupes temporels
            for time_key in f.keys():
                group = f[time_key]
                
                # Extraire les m√©triques
                for metric in group.attrs:
                    if metric not in data:
                        data[metric] = []
                    data[metric].append(group.attrs[metric])
                
                # Extraire les datasets
                for dataset_name in group.keys():
                    if dataset_name not in data:
                        data[dataset_name] = []
                    data[dataset_name].append(group[dataset_name][:])
        
        # Convertir en arrays
        for key in data:
            data[key] = np.array(data[key])
        
        return data
        
    except Exception as e:
        warnings.warn(f"Erreur chargement HDF5 : {e}")
        return {}


# ============== D√âTECTION D'ANOMALIES ==============

def detect_anomalies(data: Dict[str, np.ndarray], metrics: List[str], 
                     threshold: float = 3.0, min_duration: int = 3) -> List[Dict]:
    """
    D√©tecte les s√©quences persistantes de valeurs hors-norme.
    
    Une anomalie est une d√©viation > threshold * œÉ pendant min_duration pas.
    
    Args:
        data: donn√©es du run
        metrics: m√©triques √† analyser
        threshold: seuil en nombre d'√©carts-types
        min_duration: dur√©e minimale de persistance
    
    Returns:
        Liste d'√©v√©nements anomalies
    """
    events = []
    
    for metric in metrics:
        if metric not in data:
            continue
        
        values = data[metric]
        if len(values) < 20:  # Pas assez de donn√©es
            continue
        
        # Statistiques glissantes
        window_size = min(50, len(values) // 4)
        
        for i in range(window_size, len(values)):
            # Fen√™tre de r√©f√©rence
            window = values[i-window_size:i]
            mean_w = np.mean(window)
            std_w = np.std(window)
            
            if std_w < 1e-10:  # √âviter division par z√©ro
                continue
            
            # D√©tecter le d√©but d'une anomalie
            z_score = abs(values[i] - mean_w) / std_w
            
            if z_score > threshold:
                # Chercher la dur√©e de l'anomalie
                duration = 1
                max_z = z_score
                
                for j in range(i+1, min(i+50, len(values))):
                    z_j = abs(values[j] - mean_w) / std_w
                    if z_j > threshold:
                        duration += 1
                        max_z = max(max_z, z_j)
                    else:
                        break
                
                # Enregistrer si dur√©e suffisante
                if duration >= min_duration:
                    events.append({
                        'event_type': 'anomaly',
                        't_start': i,
                        't_end': i + duration - 1,
                        'metric': metric,
                        'value': float(max_z),
                        'severity': classify_severity(max_z, threshold)
                    })
                    
                    # Sauter √† la fin de l'anomalie
                    i += duration
    
    return events


# ============== D√âTECTION DE BIFURCATIONS ==============

def detect_spiral_bifurcations(data: Dict[str, np.ndarray], 
                               phase_metric: str = 'C(t)',
                               threshold: float = np.pi) -> List[Dict]:
    """
    Analyse les changements de phase/bifurcations dans la m√©trique d'accord spiral√©.
    
    Une bifurcation est un changement brusque de la dynamique de phase.
    
    Args:
        data: donn√©es du run
        phase_metric: m√©trique de phase √† analyser
        threshold: seuil de changement de phase
    
    Returns:
        Liste d'√©v√©nements bifurcation
    """
    events = []
    
    if phase_metric not in data:
        return events
    
    values = data[phase_metric]
    if len(values) < 10:
        return events
    
    # Calculer la d√©riv√©e de la phase
    phase_derivative = np.gradient(values)
    
    # D√©tecter les changements brusques
    for i in range(1, len(phase_derivative)-1):
        # Changement de signe de la d√©riv√©e
        if phase_derivative[i-1] * phase_derivative[i+1] < 0:
            # Amplitude du changement
            change = abs(values[i+1] - values[i-1])
            
            if change > threshold / 10:  # Seuil adaptatif
                events.append({
                    'event_type': 'bifurcation',
                    't_start': i-1,
                    't_end': i+1,
                    'metric': phase_metric,
                    'value': float(change),
                    'severity': 'medium' if change < threshold else 'high'
                })
    
    # D√©tecter aussi les sauts de phase
    phase_diff = np.diff(values)
    for i, diff in enumerate(phase_diff):
        if abs(diff) > threshold:
            events.append({
                'event_type': 'phase_jump',
                't_start': i,
                't_end': i+1,
                'metric': phase_metric,
                'value': float(abs(diff)),
                'severity': 'high'
            })
    
    return events


# ============== D√âTECTION D'√âMERGENCES HARMONIQUES ==============

def detect_harmonic_emergence(data: Dict[str, np.ndarray], 
                              signal_metric: str = 'S(t)',
                              n_harmonics: int = 5,
                              window: int = 100,
                              step: int = 10) -> List[Dict]:
    """
    Utilise une FFT glissante pour d√©tecter l'apparition de nouvelles harmoniques.
    
    Args:
        data: donn√©es du run
        signal_metric: signal √† analyser
        n_harmonics: nombre d'harmoniques principales √† suivre
        window: taille de la fen√™tre FFT
        step: pas de glissement
    
    Returns:
        Liste d'√©v√©nements harmoniques
    """
    events = []
    
    if signal_metric not in data:
        return events
    
    values = data[signal_metric]
    if len(values) < window:
        return events
    
    # FFT glissante
    prev_harmonics = None
    
    for i in range(0, len(values) - window, step):
        # Fen√™tre actuelle
        segment = values[i:i+window]
        
        # FFT
        fft_vals = np.fft.fft(segment)
        fft_abs = np.abs(fft_vals[:window//2])
        
        # Trouver les pics principaux
        peaks, properties = signal.find_peaks(fft_abs, height=np.max(fft_abs)*0.1)
        
        if len(peaks) > 0:
            # Garder les n_harmonics plus fortes
            sorted_peaks = peaks[np.argsort(properties['peak_heights'])[-n_harmonics:]]
            current_harmonics = set(sorted_peaks)
            
            if prev_harmonics is not None:
                # Nouvelles harmoniques apparues
                new_harmonics = current_harmonics - prev_harmonics
                
                if new_harmonics:
                    events.append({
                        'event_type': 'harmonic_emergence',
                        't_start': i,
                        't_end': i + window,
                        'metric': signal_metric,
                        'value': len(new_harmonics),
                        'severity': 'low' if len(new_harmonics) == 1 else 'medium'
                    })
            
            prev_harmonics = current_harmonics
    
    return events


# ============== EXPLORATION DE L'ESPACE DE PHASE ==============

def explore_phase_space(data: Dict[str, np.ndarray], 
                        metric: str = 'S(t)',
                        window: int = 50,
                        min_diagonal_length: int = 5) -> List[Dict]:
    """
    Recurrence plot : cherche les motifs r√©currents/cycles attracteurs.
    
    Args:
        data: donn√©es du run
        metric: m√©trique √† analyser
        window: taille de l'embedding
        min_diagonal_length: longueur minimale des diagonales
    
    Returns:
        Liste d'√©v√©nements de l'espace de phase
    """
    events = []
    
    if metric not in data:
        return events
    
    values = data[metric]
    if len(values) < window * 2:
        return events
    
    # Cr√©er la matrice de r√©currence
    embedding_dim = 3
    delay = 5
    
    # Embedding de Takens
    embedded = []
    for i in range(len(values) - (embedding_dim-1)*delay):
        point = [values[i + j*delay] for j in range(embedding_dim)]
        embedded.append(point)
    
    if len(embedded) < 10:
        return events
    
    embedded = np.array(embedded)
    
    # Matrice de distances
    distances = spatial.distance_matrix(embedded, embedded)
    
    # Seuil de r√©currence (10% des plus petites distances)
    threshold = np.percentile(distances.flatten(), 10)
    recurrence_matrix = distances < threshold
    
    # Chercher les structures diagonales (cycles)
    for i in range(len(recurrence_matrix) - min_diagonal_length):
        # Diagonale principale
        diagonal_length = 0
        for j in range(min(len(recurrence_matrix) - i, 50)):
            if recurrence_matrix[i+j, j]:
                diagonal_length += 1
            else:
                if diagonal_length >= min_diagonal_length:
                    events.append({
                        'event_type': 'phase_cycle',
                        't_start': i,
                        't_end': i + diagonal_length,
                        'metric': metric,
                        'value': diagonal_length,
                        'severity': 'low' if diagonal_length < 10 else 'medium'
                    })
                diagonal_length = 0
    
    return events


# ============== D√âTECTION DE MOTIFS FRACTALS ==============

def detect_fractal_patterns(data: Dict[str, np.ndarray],
                            metrics: List[str] = ['S(t)', 'C(t)', 'effort(t)'],
                            window_sizes: List[int] = [1, 10, 100],
                            threshold: float = 0.8) -> List[Dict]:
    """
    Analyse multi-√©chelles pour d√©tecter l'auto-similarit√©.
    
    D√©tecte les p√©riodes o√π le signal pr√©sente des motifs similaires
    √† diff√©rentes √©chelles temporelles.
    
    Args:
        data: donn√©es du run
        metrics: m√©triques √† analyser
        window_sizes: √©chelles √† comparer
        threshold: seuil de similarit√©
    
    Returns:
        Liste d'√©v√©nements fractals
    """
    events = []
    
    for metric in metrics:
        if metric not in data:
            continue
        
        values = data[metric]
        if len(values) < max(window_sizes) * 2:
            continue
        
        # Analyser chaque paire d'√©chelles
        for i in range(len(window_sizes)-1):
            small_window = window_sizes[i]
            large_window = window_sizes[i+1]
            
            # Parcourir le signal
            for t in range(large_window, len(values) - large_window, large_window//2):
                # Extraire les motifs √† diff√©rentes √©chelles
                small_pattern = values[t:t+small_window]
                large_pattern = values[t:t+large_window]
                
                # Sous-√©chantillonner le grand motif
                downsampled = signal.resample(large_pattern, len(small_pattern))
                
                # Calculer la corr√©lation
                if np.std(small_pattern) > 1e-10 and np.std(downsampled) > 1e-10:
                    correlation = np.corrcoef(small_pattern, downsampled)[0, 1]
                    
                    if abs(correlation) > threshold:
                        events.append({
                            'event_type': 'fractal_pattern',
                            't_start': t,
                            't_end': t + large_window,
                            'metric': metric,
                            'value': float(abs(correlation)),
                            'severity': 'medium' if abs(correlation) < 0.9 else 'high',
                            'scale': f"{small_window}/{large_window}"
                        })
        
        # Dimension fractale par box-counting
        if len(values) >= 1000:
            frac_dim = estimate_fractal_dimension(values)
            if 1.2 < frac_dim < 1.8:  # Dimension fractale non-triviale
                events.append({
                    'event_type': 'fractal_dimension',
                    't_start': 0,
                    't_end': len(values)-1,
                    'metric': metric,
                    'value': float(frac_dim),
                    'severity': 'high'
                })
    
    return events


def estimate_fractal_dimension(data: np.ndarray, max_box_size: int = 100) -> float:
    """
    Estime la dimension fractale par la m√©thode box-counting.
    """
    # Normaliser les donn√©es
    data_norm = (data - np.min(data)) / (np.max(data) - np.min(data) + 1e-10)
    
    box_sizes = []
    counts = []
    
    for box_size in range(2, min(max_box_size, len(data)//10), 2):
        # Compter les bo√Ætes occup√©es
        n_boxes = len(data) // box_size
        occupied = 0
        
        for i in range(n_boxes):
            box_data = data_norm[i*box_size:(i+1)*box_size]
            if np.ptp(box_data) > 0:
                occupied += 1
        
        if occupied > 0:
            box_sizes.append(box_size)
            counts.append(occupied)
    
    if len(box_sizes) > 3:
        # R√©gression log-log
        log_sizes = np.log(box_sizes)
        log_counts = np.log(counts)
        
        # Pente = -dimension fractale
        slope, _ = np.polyfit(log_sizes, log_counts, 1)
        return -slope
    
    return 1.0  # Dimension par d√©faut


# ============== FORMATAGE ET EXPORT ==============

def format_value_for_csv(value: Any) -> str:
    """
    Assure l'export correct de valeurs complexes dans les logs.
    """
    if isinstance(value, (list, np.ndarray)):
        return json.dumps(value.tolist() if isinstance(value, np.ndarray) else value)
    elif isinstance(value, dict):
        return json.dumps(value)
    elif isinstance(value, float):
        return f"{value:.6f}"
    else:
        return str(value)


def log_events(events: List[Dict], csv_path: str) -> None:
    """
    √âcrit le log CSV des √©mergences.
    
    Colonnes : event_type, t_start, t_end, metric, value, severity
    """
    if not events:
        return
    
    os.makedirs(os.path.dirname(csv_path) if os.path.dirname(csv_path) else ".", 
                exist_ok=True)
    
    with open(csv_path, 'w', newline='') as f:
        fieldnames = ['event_type', 't_start', 't_end', 'metric', 'value', 'severity']
        
        # Ajouter les champs suppl√©mentaires si pr√©sents
        extra_fields = set()
        for event in events:
            extra_fields.update(set(event.keys()) - set(fieldnames))
        fieldnames.extend(sorted(extra_fields))
        
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        
        for event in events:
            row = {}
            for field in fieldnames:
                if field in event:
                    row[field] = format_value_for_csv(event[field])
                else:
                    row[field] = ''
            writer.writerow(row)


def log_fractal_events(events: List[Dict], csv_path: str) -> None:
    """
    Log sp√©cifique pour les √©v√©nements fractals.
    """
    fractal_events = [e for e in events if 'fractal' in e.get('event_type', '')]
    if fractal_events:
        log_events(fractal_events, csv_path)


# ============== G√âN√âRATION DU RAPPORT ==============

def generate_report(events: List[Dict], report_path: str, 
                    run_id: str, config: Dict) -> None:
    """
    G√©n√®re un rapport Markdown d√©taill√©.
    """
    os.makedirs(os.path.dirname(report_path) if os.path.dirname(report_path) else ".", 
                exist_ok=True)
    
    with open(report_path, 'w') as f:
        # En-t√™te
        f.write(f"# Rapport d'exploration FPS\n\n")
        f.write(f"**Run ID :** {run_id}\n")
        f.write(f"**Date :** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"**Total √©v√©nements :** {len(events)}\n\n")
        
        # R√©sum√© par type
        f.write("## R√©sum√© par type d'√©v√©nement\n\n")
        event_counts = count_events_by_type(events)
        
        for event_type, count in event_counts.items():
            f.write(f"- **{event_type}** : {count} √©v√©nements\n")
        f.write("\n")
        
        # D√©tails par type
        for event_type in event_counts:
            type_events = [e for e in events if e['event_type'] == event_type]
            
            f.write(f"## {event_type.replace('_', ' ').title()}\n\n")
            
            # Top 5 par s√©v√©rit√©/valeur
            top_events = sorted(type_events, key=lambda x: x.get('value', 0), reverse=True)[:5]
            
            for i, event in enumerate(top_events, 1):
                f.write(f"### {i}. t={event['t_start']}-{event['t_end']}\n")
                f.write(f"- **M√©trique :** {event['metric']}\n")
                f.write(f"- **Valeur :** {event['value']:.4f}\n")
                f.write(f"- **S√©v√©rit√© :** {event['severity']}\n")
                
                # Champs suppl√©mentaires
                for key, value in event.items():
                    if key not in ['event_type', 't_start', 't_end', 'metric', 'value', 'severity']:
                        f.write(f"- **{key} :** {value}\n")
                f.write("\n")
        
        # Section sp√©ciale pour les motifs fractals
        fractal_events = [e for e in events if 'fractal' in e.get('event_type', '')]
        if fractal_events:
            f.write("## Motifs fractals d√©tect√©s\n\n")
            f.write(f"**Nombre total :** {len(fractal_events)}\n\n")
            
            # Grouper par m√©trique
            by_metric = defaultdict(list)
            for event in fractal_events:
                by_metric[event['metric']].append(event)
            
            for metric, metric_events in by_metric.items():
                f.write(f"### {metric}\n")
                f.write(f"- Patterns d√©tect√©s : {len(metric_events)}\n")
                
                # Statistiques de corr√©lation
                correlations = [e['value'] for e in metric_events if 'pattern' in e['event_type']]
                if correlations:
                    f.write(f"- Corr√©lation moyenne : {np.mean(correlations):.3f}\n")
                    f.write(f"- Corr√©lation max : {np.max(correlations):.3f}\n")
                
                # Dimension fractale si pr√©sente
                dim_events = [e for e in metric_events if e['event_type'] == 'fractal_dimension']
                if dim_events:
                    f.write(f"- Dimension fractale : {dim_events[0]['value']:.3f}\n")
                f.write("\n")
        
        # Configuration utilis√©e
        f.write("## Configuration d'exploration\n\n")
        f.write("```json\n")
        f.write(json.dumps(config.get('exploration', {}), indent=2))
        f.write("\n```\n")


# ============== UTILITAIRES ==============

def load_config_for_exploration() -> Dict:
    """
    Charge la configuration depuis config.json.
    """
    try:
        with open('config.json', 'r') as f:
            return json.load(f)
    except:
        # Configuration par d√©faut
        return {
            'exploration': {
                'metrics': ['S(t)', 'C(t)', 'effort(t)'],
                'window_sizes': [1, 10, 100],
                'fractal_threshold': 0.8,
                'detect_fractal_patterns': True,
                'detect_anomalies': True,
                'detect_harmonics': True,
                'anomaly_threshold': 3.0,
                'min_duration': 3
            }
        }


def extract_run_id(file_path: str) -> str:
    """
    Extrait le run_id du nom de fichier.
    """
    basename = os.path.basename(file_path)
    # Essayer diff√©rents patterns
    if 'run_' in basename:
        parts = basename.split('run_')[1].split('.')[0]
        return f"run_{parts}"
    else:
        return basename.split('.')[0]


def count_events_by_type(events: List[Dict]) -> Dict[str, int]:
    """
    Compte les √©v√©nements par type.
    """
    counts = defaultdict(int)
    for event in events:
        counts[event['event_type']] += 1
    return dict(counts)


def classify_severity(value: float, threshold: float) -> str:
    """
    Classifie la s√©v√©rit√© d'un √©v√©nement.
    """
    if value < threshold * 1.5:
        return 'low'
    elif value < threshold * 3:
        return 'medium'
    else:
        return 'high'


# ============== TESTS ET VALIDATION ==============

if __name__ == "__main__":
    """
    Tests du module explore.py
    """
    print("=== Tests du module explore.py ===\n")
    
    # Cr√©er des donn√©es de test
    print("Test 1 - G√©n√©ration de donn√©es synth√©tiques:")
    t = np.linspace(0, 100, 1000)
    
    # Signal avec anomalies, bifurcations et fractales
    S_test = np.sin(2 * np.pi * t / 10)  # Signal de base
    S_test[200:220] += 5.0  # Anomalie
    S_test[500:] += 0.5 * np.sin(2 * np.pi * t[500:] / 3)  # Nouvelle harmonique
    
    # Ajouter du bruit fractal
    for scale in [1, 10, 100]:
        S_test += 0.1 / scale * np.sin(2 * np.pi * t * scale)
    
    C_test = np.cos(2 * np.pi * t / 15)
    C_test[400] += 3.0  # Saut de phase
    
    effort_test = 0.5 + 0.3 * np.sin(2 * np.pi * t / 20)
    effort_test[300:350] = 2.5  # Effort √©lev√©
    
    test_data = {
        'S(t)': S_test,
        'C(t)': C_test,
        'effort(t)': effort_test
    }
    
    print("  ‚úì Donn√©es synth√©tiques cr√©√©es")
    
    # Test d√©tection d'anomalies
    print("\nTest 2 - D√©tection d'anomalies:")
    anomalies = detect_anomalies(test_data, ['S(t)', 'effort(t)'], 3.0, 3)
    print(f"  ‚Üí {len(anomalies)} anomalies d√©tect√©es")
    
    # Test bifurcations
    print("\nTest 3 - D√©tection de bifurcations:")
    bifurcations = detect_spiral_bifurcations(test_data, 'C(t)', np.pi)
    print(f"  ‚Üí {len(bifurcations)} bifurcations d√©tect√©es")
    
    # Test harmoniques
    print("\nTest 4 - √âmergences harmoniques:")
    harmonics = detect_harmonic_emergence(test_data, 'S(t)', 5, 100, 10)
    print(f"  ‚Üí {len(harmonics)} √©mergences harmoniques")
    
    # Test fractales
    print("\nTest 5 - Motifs fractals:")
    fractals = detect_fractal_patterns(test_data, ['S(t)'], [1, 10, 100], 0.7)
    print(f"  ‚Üí {len(fractals)} motifs fractals")
    
    # Test rapport
    print("\nTest 6 - G√©n√©ration rapport:")
    all_test_events = anomalies + bifurcations + harmonics + fractals
    
    os.makedirs("test_output", exist_ok=True)
    generate_report(all_test_events, "test_output/test_report.md", "test_run", {})
    print("  ‚úì Rapport g√©n√©r√© : test_output/test_report.md")
    
    print("\n‚úÖ Module explore.py pr√™t √† r√©v√©ler l'invisible!")
