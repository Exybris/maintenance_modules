"""
main.py - Orchestrateur principal du pipeline FPS
Version exhaustive conforme √† la feuille de route FPS V1.3
---------------------------------------------------------------
Ce module orchestre TOUS les autres modules du pipeline FPS
sans jamais les court-circuiter, en respectant parfaitement
leurs interfaces et structures de donn√©es.

Fonctionnalit√©s :
- Validation compl√®te via validate_config.py
- Ex√©cution via simulate.py (FPS/Kuramoto/Neutral)
- Exploration via explore.py
- Analyse batch via analyze.py
- Visualisation via visualize.py
- Gestion compl√®te des erreurs et donn√©es
- Pipeline complet end-to-end

PRINCIPE : Ce module est un ORCHESTRATEUR pur.
Il ne fait QUE coordonner les autres modules.
Aucune logique m√©tier n'est impl√©ment√©e ici.

(c) 2025 Gepetto & Andr√©a Gadal & Claude üåÄ
"""

import argparse
import os
import json
import sys
import traceback
from datetime import datetime
import glob
from collections import defaultdict
from typing import Dict, List, Optional, Any, Tuple

# Imports conditionnels avec gestion d'erreurs
try:
    import numpy as np
except ImportError:
    print("‚ùå NumPy non install√©. Installez avec : pip install numpy")
    sys.exit(1)

try:
    import matplotlib
    matplotlib.use('Agg')  # Backend sans interface pour serveurs
    import matplotlib.pyplot as plt
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è  Matplotlib non install√©. Visualisations d√©sactiv√©es.")
    print("   Pour activer : pip install matplotlib")
    MATPLOTLIB_AVAILABLE = False

# Import des modules FPS avec v√©rification
FPS_MODULES = {}
for module_name in ['validate_config', 'simulate', 'explore', 'analyze', 'visualize', 'utils']:
    try:
        FPS_MODULES[module_name] = __import__(module_name)
        print(f"‚úì Module {module_name} import√©")
    except ImportError as e:
        print(f"‚ùå Module {module_name} manquant : {e}")
        sys.exit(1)


def check_prerequisites() -> bool:
    """
    V√©rifie que tous les pr√©requis sont satisfaits.
    
    Returns:
        bool: True si tout est OK
    """
    print("\nüîß V√©rification des pr√©requis...")
    
    # V√©rifier les modules FPS
    required_modules = ['validate_config', 'simulate', 'explore', 'analyze', 'utils']
    if MATPLOTLIB_AVAILABLE:
        required_modules.append('visualize')
    
    missing = []
    for module in required_modules:
        if module not in FPS_MODULES:
            missing.append(module)
    
    if missing:
        print(f"‚ùå Modules manquants : {missing}")
        return False
    
    # V√©rifier les d√©pendances Python
    try:
        import scipy
        print("‚úì SciPy disponible")
    except ImportError:
        print("‚ö†Ô∏è  SciPy recommand√© mais non critique")
    
    try:
        import pandas
        print("‚úì Pandas disponible")
    except ImportError:
        print("‚ö†Ô∏è  Pandas recommand√© pour l'analyse")
    
    print("‚úÖ Pr√©requis valid√©s")
    return True


def validate_configuration(config_path: str) -> Tuple[bool, Dict]:
    """
    Valide la configuration via validate_config.py.
    
    Args:
        config_path: chemin vers config.json
    
    Returns:
        Tuple[bool, Dict]: (valid, config_dict)
    """
    print(f"\nüìã Validation de la configuration : {config_path}")
    
    if not os.path.exists(config_path):
        print(f"‚ùå Fichier de configuration non trouv√© : {config_path}")
        return False, {}
    
    try:
        # Utiliser validate_config.py
        print(f"  ‚Üí Appel validate_config({config_path})...")
        errors, warnings = FPS_MODULES['validate_config'].validate_config(config_path)
        
        print(f"  ‚Üí Validation termin√©e. Errors: {len(errors) if errors else 0}, Warnings: {len(warnings) if warnings else 0}")
        
        # Debug : afficher le type et contenu des erreurs
        if errors:
            print(f"  ‚Üí Type errors: {type(errors)}")
            print(f"  ‚Üí Contenu errors: {errors}")
        
        # Afficher les erreurs
        if errors:
            print("‚ùå Erreurs de configuration :")
            for i, error in enumerate(errors):
                print(f"  - [{i}] {error}")
            return False, {}
        
        # Afficher les warnings
        if warnings:
            print("‚ö†Ô∏è  Avertissements :")
            for i, warning in enumerate(warnings):
                print(f"  - [{i}] {warning}")
        
        # Charger la config si validation OK
        with open(config_path, 'r') as f:
            config = json.load(f)
        
        print("‚úÖ Configuration valid√©e")
        return True, config
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la validation : {e}")
        import traceback
        traceback.print_exc()
        return False, {}


def setup_environment(config: Dict) -> Dict[str, str]:
    """
    Configure l'environnement de travail via utils.py.
    
    Args:
        config: configuration valid√©e
    
    Returns:
        Dict: chemins des dossiers cr√©√©s
    """
    print("\nüìÅ Configuration de l'environnement...")
    
    try:
        # Utiliser utils.py pour cr√©er la structure
        dirs = FPS_MODULES['utils'].setup_directories("fps_pipeline_output")
        
        # Logger la configuration
        run_id = FPS_MODULES['utils'].generate_run_id("pipeline")
        FPS_MODULES['utils'].log_config_and_meta(config, run_id, dirs['configs'])
        
        dirs['pipeline_run_id'] = run_id
        
        print(f"‚úÖ Environnement configur√© : {dirs['base']}")
        return dirs
        
    except Exception as e:
        print(f"‚ùå Erreur configuration environnement : {e}")
        raise


def execute_simulations(config_path: str, config: Dict, dirs: Dict) -> Dict[str, Any]:
    """
    Execute les simulations via simulate.py.
    
    Args:
        config_path: chemin config
        config: configuration
        dirs: dossiers de travail
    
    Returns:
        Dict: r√©sultats des simulations
    """
    print("\nüî¨ Ex√©cution des simulations...")
    
    results = {}
    modes = ['FPS']
    
    # Ajouter les modes contr√¥les selon config
    if config.get('analysis', {}).get('compare_kuramoto', True):
        modes.append('Kuramoto')
    modes.append('neutral')
    
    for mode in modes:
        print(f"\n  ‚Üí Simulation {mode}...")
        
        try:
            # Valider la config avant l'ex√©cution
            if mode == 'FPS':
                # V√©rifications sp√©cifiques FPS
                N = config['system']['N']
                if N <= 0:
                    raise ValueError(f"N doit √™tre > 0, re√ßu: {N}")
            
            # Utiliser simulate.py directement
            result = FPS_MODULES['simulate'].run_simulation(config_path, mode)
            
            # Copier le fichier log dans le dossier du pipeline
            if 'logs' in result and isinstance(result['logs'], str):
                original_log = result['logs']
                if os.path.exists(original_log):
                    # Copier dans le dossier logs du pipeline
                    import shutil
                    pipeline_log_dir = os.path.join(dirs['base'], 'logs')
                    os.makedirs(pipeline_log_dir, exist_ok=True)
                    
                    new_log_path = os.path.join(pipeline_log_dir, os.path.basename(original_log))
                    shutil.copy2(original_log, new_log_path)
                    result['copied_log'] = new_log_path
                    print(f"    ‚úì {mode} termin√© : {result['run_id']} (log copi√©)")
                else:
                    print(f"    ‚úì {mode} termin√© : {result['run_id']} (log non trouv√©)")
            else:
                print(f"    ‚úì {mode} termin√© : {result['run_id']}")
            
            results[mode.lower()] = result
            
        except Exception as e:
            print(f"    ‚ùå Erreur {mode} : {e}")
            import traceback
            traceback.print_exc()
            results[mode.lower()] = {'status': 'error', 'error': str(e)}
    
    return results


def run_exploration_analysis(results: Dict, config: Dict, dirs: Dict) -> Dict[str, Any]:
    """
    Lance l'exploration via explore.py.
    
    Args:
        results: r√©sultats des simulations
        config: configuration
        dirs: dossiers
    
    Returns:
        Dict: r√©sultats d'exploration
    """
    print("\nüîç Exploration et d√©tection d'√©mergences...")
    
    exploration_results = {}
    
    for mode, result in results.items():
        if result.get('status') == 'error':
            continue
        
        print(f"  ‚Üí Exploration {mode}...")
        
        try:
            # Utiliser le log copi√© ou original
            log_path = result.get('copied_log', result.get('logs'))
            if not log_path:
                print(f"    ‚ö†Ô∏è Log non trouv√© pour {mode}")
                continue
                
            # V√©rifier si c'est un fichier qui existe
            if isinstance(log_path, str) and not os.path.exists(log_path):
                # Essayer dans le dossier logs
                if not log_path.startswith('logs/'):
                    log_path = os.path.join('logs', os.path.basename(log_path))
                if not os.path.exists(log_path):
                    print(f"    ‚ö†Ô∏è Fichier log non trouv√©: {log_path}")
                    continue
            
            # Cr√©er dossier de sortie pour ce mode
            output_dir = os.path.join(dirs['reports'], f"{mode}_exploration")
            
            # Utiliser explore.py
            exploration_result = FPS_MODULES['explore'].run_exploration(
                log_path, output_dir, config
            )
            
            exploration_results[mode] = exploration_result
            print(f"    ‚úì {mode} explor√© : {exploration_result.get('total_events', 0)} √©v√©nements")
            
        except Exception as e:
            print(f"    ‚ùå Erreur exploration {mode} : {e}")
            exploration_results[mode] = {'status': 'error', 'error': str(e)}
    
    return exploration_results


def run_batch_analysis(config_path: str, config: Dict, dirs: Dict) -> Optional[Dict]:
    """
    Execute l'analyse batch via analyze.py si configur√©.
    
    Args:
        config_path: chemin config
        config: configuration
        dirs: dossiers
    
    Returns:
        Optional[Dict]: r√©sultats batch ou None
    """
    batch_size = config.get('validation', {}).get('batch_size', 1)
    if batch_size <= 1:
        print("\nüìä Analyse batch d√©sactiv√©e (batch_size <= 1)")
        return None
    
    print(f"\nüìä Analyse batch : {batch_size} runs...")
    
    try:
        # Ex√©cuter le batch dans le dossier logs du pipeline
        batch_log_dir = dirs['logs']
        batch_logs = []
        
        # Ex√©cuter les simulations une par une
        for i in range(batch_size):
            print(f"  ‚Üí Run {i+1}/{batch_size}...")
            
            # Modifier la seed pour chaque run
            config_copy = json.loads(json.dumps(config))  # Deep copy
            config_copy['system']['seed'] = config['system']['seed'] + i
            
            # Cr√©er un fichier config temporaire
            temp_config_path = os.path.join(dirs['configs'], f'config_batch_{i}.json')
            with open(temp_config_path, 'w') as f:
                json.dump(config_copy, f, indent=2)
            
            # Ex√©cuter la simulation
            result = FPS_MODULES['simulate'].run_simulation(temp_config_path, 'FPS')
            
            if result.get('logs'):
                # Copier le log dans le dossier du pipeline
                original_log = result['logs']
                if os.path.exists(original_log):
                    new_log_path = os.path.join(batch_log_dir, f'batch_run_{i}_{os.path.basename(original_log)}')
                    import shutil
                    shutil.copy2(original_log, new_log_path)
                    batch_logs.append(new_log_path)
                    print(f"    ‚úì Log copi√© : {os.path.basename(new_log_path)}")
        
        if len(batch_logs) >= 2:
            print(f"  ‚Üí Analyse de {len(batch_logs)} runs...")
            
            # Utiliser analyze.py avec les bons chemins
            analysis_result = FPS_MODULES['analyze'].analyze_criteria_and_refine(
                batch_logs, config
            )
            
            # Sauvegarder la config raffin√©e
            if analysis_result.get('refinements'):
                refined_path = os.path.join(dirs['configs'], 'config_refined.json')
                with open(refined_path, 'w') as f:
                    json.dump(analysis_result['updated_config'], f, indent=2)
                print(f"  ‚úì Config raffin√©e : {refined_path}")
            
            return analysis_result
        else:
            print("  ‚ö†Ô∏è Pas assez de logs cr√©√©s pour l'analyse")
            return None
            
    except Exception as e:
        print(f"  ‚ùå Erreur analyse batch : {e}")
        import traceback
        traceback.print_exc()
        return None

def generate_visualizations(results: Dict, config: Dict, dirs: Dict) -> Dict[str, str]:
    """
    G√©n√®re les visualisations via visualize.py.
    
    Args:
        results: r√©sultats des simulations
        config: configuration
        dirs: dossiers
    
    Returns:
        Dict: chemins des figures g√©n√©r√©es
    """
    if not MATPLOTLIB_AVAILABLE:
        print("\nüìà Visualisations d√©sactiv√©es (matplotlib manquant)")
        return {}
    
    print("\nüìà G√©n√©ration des visualisations...")
    
    figures_paths = {}
    
    try:
        # V√©rifier qu'on a au moins FPS
        fps_result = results.get('fps')
        if not fps_result or fps_result.get('status') == 'error':
            print("  ‚ö†Ô∏è Pas de r√©sultats FPS valides pour visualisation")
            return {}
        
        # Pr√©parer les donn√©es temporelles pour visualize.py
        # Selon simulate.py, on a S_history, cpu_steps, etc.
        
        # 1. √âvolution du signal S(t)
        if 'S_history' in fps_result and len(fps_result['S_history']) > 0:
            print("  ‚Üí Signal evolution...")
            
            # Reconstruire le temps
            T = config.get('system', {}).get('T', 100)
            dt = config.get('system', {}).get('dt', 0.05)
            t_array = np.arange(0, len(fps_result['S_history']) * dt, dt)[:len(fps_result['S_history'])]
            
            fig1 = FPS_MODULES['visualize'].plot_signal_evolution(
                t_array, 
                np.array(fps_result['S_history']),
                "√âvolution FPS - Signal S(t)"
            )
            path1 = os.path.join(dirs['figures'], 'signal_evolution_fps.png')
            fig1.savefig(path1, dpi=150, bbox_inches='tight')
            figures_paths['signal_evolution'] = path1
            plt.close(fig1)
        
        # 2. Dashboard m√©triques (si on a l'historique)
        if 'history' in fps_result:
            print("  ‚Üí Metrics dashboard...")
            
            fig2 = FPS_MODULES['visualize'].plot_metrics_dashboard(fps_result['history'])
            path2 = os.path.join(dirs['figures'], 'metrics_dashboard.png')
            fig2.savefig(path2, dpi=150, bbox_inches='tight')
            figures_paths['metrics_dashboard'] = path2
            plt.close(fig2)
        
        # 3. Comparaison avec Kuramoto si disponible
        kuramoto_result = results.get('kuramoto')
        if kuramoto_result and kuramoto_result.get('status') != 'error':
            print("  ‚Üí Comparaison FPS vs Kuramoto...")
            
            fig3 = FPS_MODULES['visualize'].plot_fps_vs_kuramoto(fps_result, kuramoto_result)
            path3 = os.path.join(dirs['figures'], 'fps_vs_kuramoto.png')
            fig3.savefig(path3, dpi=150, bbox_inches='tight')
            figures_paths['comparison'] = path3
            plt.close(fig3)
        
        # 4. Grille empirique
        print("  ‚Üí Grille empirique...")
        scores = calculate_empirical_scores(fps_result.get('metrics', {}))
        fig4 = FPS_MODULES['visualize'].create_empirical_grid(scores)
        path4 = os.path.join(dirs['figures'], 'empirical_grid.png')
        fig4.savefig(path4, dpi=150, bbox_inches='tight')
        figures_paths['empirical_grid'] = path4
        plt.close(fig4)
        
        # 5. Matrice crit√®res-termes
        print("  ‚Üí Matrice crit√®res-termes...")
        mapping = get_criteria_terms_mapping()
        fig5 = FPS_MODULES['visualize'].generate_correlation_matrix(mapping)
        path5 = os.path.join(dirs['figures'], 'criteria_terms_matrix.png')
        fig5.savefig(path5, dpi=150, bbox_inches='tight')
        figures_paths['correlation_matrix'] = path5
        plt.close(fig5)
        
        print(f"  ‚úì {len(figures_paths)} visualisations g√©n√©r√©es")
        
    except Exception as e:
        print(f"  ‚ùå Erreur visualisations : {e}")
        traceback.print_exc()
    
    return figures_paths


def generate_final_report(results: Dict, exploration_results: Dict, 
                         analysis_result: Optional[Dict], config: Dict, 
                         dirs: Dict) -> str:
    """
    G√©n√®re le rapport final HTML via visualize.py.
    
    Args:
        results: r√©sultats simulations
        exploration_results: r√©sultats exploration
        analysis_result: r√©sultats analyse batch
        config: configuration
        dirs: dossiers
    
    Returns:
        str: chemin du rapport
    """
    print("\nüìÑ G√©n√©ration du rapport final...")
    
    try:
        # Agr√©gation des donn√©es pour visualize.py
        all_data = {
            'fps_result': results.get('fps', {}),
            'kuramoto_result': results.get('kuramoto', {}),
            'neutral_result': results.get('neutral', {}),
            'exploration_results': exploration_results,
            'analysis_result': analysis_result,
            'config': config,
            'metrics_summary': results.get('fps', {}).get('metrics', {}),
            'emergence_summary': count_emergence_events(exploration_results),
            'pipeline_metadata': {
                'timestamp': datetime.now().isoformat(),
                'run_id': dirs.get('pipeline_run_id', 'unknown'),
                'fps_version': '1.3'
            }
        }
        
        # Utiliser visualize.py
        report_path = os.path.join(dirs['reports'], 'rapport_complet_fps.html')
        FPS_MODULES['visualize'].export_html_report(all_data, report_path)
        
        print(f"  ‚úì Rapport g√©n√©r√© : {report_path}")
        return report_path
        
    except Exception as e:
        print(f"  ‚ùå Erreur g√©n√©ration rapport : {e}")
        # Cr√©er un rapport minimal en cas d'erreur
        minimal_report = create_minimal_report(results, dirs)
        return minimal_report


def create_minimal_report(results: Dict, dirs: Dict) -> str:
    """
    Cr√©e un rapport minimal en cas d'erreur.
    """
    report_path = os.path.join(dirs['reports'], 'rapport_minimal.txt')
    
    with open(report_path, 'w') as f:
        f.write("RAPPORT FPS - VERSION MINIMALE\n")
        f.write("=" * 40 + "\n\n")
        f.write(f"G√©n√©r√© le : {datetime.now()}\n\n")
        
        f.write("R√âSULTATS DES SIMULATIONS :\n")
        for mode, result in results.items():
            if result.get('status') == 'error':
                f.write(f"  {mode}: ERREUR - {result.get('error', 'inconnue')}\n")
            else:
                f.write(f"  {mode}: OK - Run ID {result.get('run_id', 'unknown')}\n")
        
        f.write(f"\nTous les fichiers dans : {dirs['base']}\n")
    
    return report_path


# ============== FONCTIONS HELPER ==============

def calculate_empirical_scores(metrics: Dict) -> Dict[str, int]:
    """
    Calcule les scores 1-5 pour la grille empirique.
    
    Cette fonction utilise les m√©triques calcul√©es par les autres modules
    pour √©valuer chaque crit√®re selon l'√©chelle FPS.
    """
    scores = {}
    
    # Logique bas√©e sur les seuils de config.json et les m√©triques
    # (Ces seuils peuvent √™tre ajust√©s selon l'exp√©rience)
    
    # Stabilit√© (bas√©e sur std_S et max_median_ratio)
    std_s = metrics.get('std_S', float('inf'))
    if std_s < 0.5:
        scores['Stabilit√©'] = 5
    elif std_s < 1.0:
        scores['Stabilit√©'] = 4
    elif std_s < 2.0:
        scores['Stabilit√©'] = 3
    else:
        scores['Stabilit√©'] = 2
    
    # R√©gulation (bas√©e sur final_mean_abs_error)
    error = metrics.get('final_mean_abs_error', float('inf'))
    if error < 0.1:
        scores['R√©gulation'] = 5
    elif error < 0.5:
        scores['R√©gulation'] = 4
    elif error < 1.0:
        scores['R√©gulation'] = 3
    else:
        scores['R√©gulation'] = 2
    
    # Fluidit√© (bas√©e sur final_variance_d2S)
    var_d2s = metrics.get('final_variance_d2S', float('inf'))
    if var_d2s < 0.001:
        scores['Fluidit√©'] = 5
    elif var_d2s < 0.01:
        scores['Fluidit√©'] = 4
    elif var_d2s < 0.1:
        scores['Fluidit√©'] = 3
    else:
        scores['Fluidit√©'] = 2
    
    # R√©silience (bas√©e sur resilience_t_retour)
    t_retour = metrics.get('resilience_t_retour', float('inf'))
    if t_retour < 1.0:
        scores['R√©silience'] = 5
    elif t_retour < 2.0:
        scores['R√©silience'] = 4
    elif t_retour < 5.0:
        scores['R√©silience'] = 3
    else:
        scores['R√©silience'] = 2
    
    # Innovation (bas√©e sur final_entropy_S)
    entropy = metrics.get('final_entropy_S', 0)
    if entropy > 0.8:
        scores['Innovation'] = 5
    elif entropy > 0.6:
        scores['Innovation'] = 4
    elif entropy > 0.4:
        scores['Innovation'] = 3
    else:
        scores['Innovation'] = 2
    
    # Co√ªt CPU (bas√© sur mean_cpu_step)
    cpu = metrics.get('mean_cpu_step', float('inf'))
    if cpu < 0.001:
        scores['Co√ªt CPU'] = 5
    elif cpu < 0.01:
        scores['Co√ªt CPU'] = 4
    elif cpu < 0.1:
        scores['Co√ªt CPU'] = 3
    else:
        scores['Co√ªt CPU'] = 2
    
    # Effort interne (bas√© sur mean_effort)
    effort = metrics.get('mean_effort', float('inf'))
    if effort < 0.5:
        scores['Effort interne'] = 5
    elif effort < 1.0:
        scores['Effort interne'] = 4
    elif effort < 2.0:
        scores['Effort interne'] = 3
    else:
        scores['Effort interne'] = 2
    
    return scores


def get_criteria_terms_mapping() -> Dict[str, List[str]]:
    """
    Retourne le mapping crit√®res-termes FPS pour la matrice de corr√©lation.
    
    Cette fonction d√©finit la correspondance entre les crit√®res empiriques
    et les termes math√©matiques du syst√®me FPS.
    """
    return {
        'Stabilit√©': ['S(t)', 'C(t)', 'œÜ‚Çô(t)', 'L(t)', 'max_median_ratio'],
        'R√©gulation': ['F‚Çô(t)', 'G(x)', 'Œ≥(t)', 'A‚Çô(t)', 'mean_abs_error'],
        'Fluidit√©': ['Œ≥‚Çô(t)', 'œÉ(x)', 'env‚Çô(x,t)', 'Œº‚Çô(t)', 'variance_d2S'],
        'R√©silience': ['A‚Çô(t)', 'G(x,t)', 'effort(t)', 't_retour'],
        'Innovation': ['A_spiral(t)', 'E‚Çô(t)', 'r(t)', 'entropy_S'],
        'Co√ªt CPU': ['cpu_step(t)', 'N', 'T'],
        'Effort interne': ['effort(t)', 'd_effort/dt', 'mean_high_effort']
    }


def count_emergence_events(exploration_results: Dict) -> Dict[str, int]:
    """
    Compte les √©v√©nements d'√©mergence d√©tect√©s par explore.py.
    
    Args:
        exploration_results: r√©sultats d'exploration
    
    Returns:
        Dict: comptage par type d'√©v√©nement
    """
    summary = defaultdict(int)
    
    for mode, result in exploration_results.items():
        if result.get('status') == 'error':
            continue
        
        events = result.get('events', [])
        for event in events:
            event_type = event.get('event_type', 'unknown')
            summary[event_type] += 1
    
    return dict(summary)


# ============== PIPELINE PRINCIPAL ==============

def run_complete_pipeline(config_path: str, parallel: bool = False) -> bool:
    """
    Execute le pipeline complet FPS avec orchestration parfaite.
    
    Args:
        config_path: chemin vers config.json
        parallel: utilisation du parall√©lisme pour batch
    
    Returns:
        bool: True si succ√®s complet
    """
    print("\nüåÄ PIPELINE FPS COMPLET - ORCHESTRATION V1.3 üåÄ")
    print("=" * 60)
    
    try:
        # 1. V√©rifications pr√©alables
        if not check_prerequisites():
            return False
        
        # 2. Validation configuration via validate_config.py
        valid, config = validate_configuration(config_path)
        if not valid:
            return False
        
        # 3. Setup environnement via utils.py
        dirs = setup_environment(config)
        
        # 4. Ex√©cution simulations via simulate.py
        results = execute_simulations(config_path, config, dirs)

        # Rapport de comparaison
        if all(mode in results for mode in ['fps', 'kuramoto', 'neutral']):
            # Importer le module de comparaison
            import compare_modes
    
            # G√©n√©rer le rapport de comparaison
            comparison_path = os.path.join(dirs['reports'], 'comparison_fps_vs_controls.json')
            comparison_report = compare_modes.export_comparison_report(
                results['fps'],
                results['kuramoto'], 
                results['neutral'],
                comparison_path
            )
    
            print(f"\nüìä Rapport de comparaison g√©n√©r√© :")
            print(f"  JSON : {comparison_path}")
            print(f"  TXT  : {comparison_path.replace('.json', '.txt')}")
            print(f"  Verdict : {comparison_report['summary']['overall_verdict']}")
        
        # V√©rifier qu'on a au moins un r√©sultat valide
        valid_results = {k: v for k, v in results.items() if v.get('status') != 'error'}
        if not valid_results:
            print("‚ùå Aucune simulation n'a r√©ussi")
            return False
        
        # 5. Exploration via explore.py
        exploration_results = run_exploration_analysis(results, config, dirs)
        
        # 6. Analyse batch via analyze.py (optionnel)
        analysis_result = run_batch_analysis(config_path, config, dirs)
        
        # 7. Visualisations via visualize.py
        figures_paths = generate_visualizations(results, config, dirs)
        
        # 8. Rapport final via visualize.py
        report_path = generate_final_report(
            results, exploration_results, analysis_result, config, dirs
        )
        
        # 9. R√©sum√© final
        print("\n" + "=" * 60)
        print("‚úÖ PIPELINE TERMIN√â AVEC SUCC√àS !")
        print("=" * 60)
        print(f"üìÇ Dossier principal : {dirs['base']}")
        print(f"üìÑ Rapport complet : {report_path}")
        print(f"üìä {len(valid_results)} simulations r√©ussies")
        print(f"üìà {len(figures_paths)} visualisations g√©n√©r√©es")
        
        if analysis_result and analysis_result.get('refinements'):
            print(f"üîß {len(analysis_result['refinements'])} raffinements appliqu√©s")
        
        print("\nüåÄ La danse FPS s'ach√®ve en harmonie ! üåÄ")
        return True
        
    except Exception as e:
        print(f"\n‚ùå ERREUR CRITIQUE PIPELINE : {e}")
        traceback.print_exc()
        return False


def main():
    """Point d'entr√©e principal avec interface CLI compl√®te."""
    
    parser = argparse.ArgumentParser(
        description='FPS - Fractal Pulsating Spiral v1.3 - Pipeline Complet',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'utilisation:

  # Pipeline complet (recommand√©)
  python main.py complete --config config.json

  # Run simple avec mode sp√©cifique
  python main.py run --config config.json --mode FPS

  # Batch de simulations en parall√®le
  python main.py batch --config config.json --parallel

  # Analyse d'un batch existant
  python main.py analyze --config config.json

  # Comparaison FPS vs Kuramoto seulement
  python main.py compare --config config.json

  # Validation seule de la configuration
  python main.py validate --config config.json

        """
    )
    
    parser.add_argument('action', 
                        choices=['complete', 'run', 'batch', 'analyze', 'compare', 'validate'],
                        help='Action √† effectuer')
    parser.add_argument('--config', default='config.json',
                        help='Fichier de configuration (d√©faut: config.json)')
    parser.add_argument('--mode', default='FPS', 
                        choices=['FPS', 'Kuramoto', 'neutral'],
                        help='Mode de simulation pour run simple')
    parser.add_argument('--parallel', action='store_true',
                        help='Ex√©cution parall√®le pour batch')
    parser.add_argument('--verbose', '-v', action='store_true',
                        help='Affichage d√©taill√©')
    
    args = parser.parse_args()
    
    # Configuration du niveau de d√©tail
    if args.verbose:
        print("Mode verbose activ√©")
    
    # Ex√©cution selon l'action demand√©e
    try:
        if args.action == 'complete':
            # Pipeline complet - RECOMMAND√â
            success = run_complete_pipeline(args.config, args.parallel)
            sys.exit(0 if success else 1)
            
        elif args.action == 'validate':
            # Validation seule
            valid, config = validate_configuration(args.config)
            if valid:
                print("‚úÖ Configuration valide")
                sys.exit(0)
            else:
                print("‚ùå Configuration invalide")
                sys.exit(1)
                
        elif args.action == 'run':
            # Run simple via simulate.py
            if not check_prerequisites():
                sys.exit(1)
            
            valid, config = validate_configuration(args.config)
            if not valid:
                sys.exit(1)
            
            print(f"\nüî¨ Simulation {args.mode}...")
            result = FPS_MODULES['simulate'].run_simulation(args.config, args.mode)
            print(f"‚úÖ Termin√© : {result['run_id']}")
            
        elif args.action == 'batch':
            # Batch via utils.py
            if not check_prerequisites():
                sys.exit(1)
            
            valid, config = validate_configuration(args.config)
            if not valid:
                sys.exit(1)
            
            batch_size = config.get('validation', {}).get('batch_size', 5)
            configs = [args.config] * batch_size
            
            print(f"\nüîÑ Batch de {batch_size} simulations...")
            results = FPS_MODULES['utils'].batch_runner(configs, args.parallel)
            
            success_count = sum(1 for r in results if r.get('status') == 'success')
            print(f"‚úÖ Batch termin√© : {success_count}/{len(results)} succ√®s")
            
        elif args.action == 'analyze':
            # Analyse via analyze.py
            if not check_prerequisites():
                sys.exit(1)
            
            valid, config = validate_configuration(args.config)
            if not valid:
                sys.exit(1)
            
            # Chercher les logs r√©cents
            logs = sorted(glob.glob('logs/run_*.csv'))[-5:]
            if len(logs) < 2:
                print("‚ùå Pas assez de logs pour l'analyse (minimum 2)")
                sys.exit(1)
            
            print(f"\nüìä Analyse de {len(logs)} runs...")
            result = FPS_MODULES['analyze'].analyze_criteria_and_refine(logs, config)
            
            if result.get('refinements'):
                print(f"‚úÖ {len(result['refinements'])} raffinements appliqu√©s")
            else:
                print("‚úÖ Aucun raffinement n√©cessaire")
                
        elif args.action == 'compare':
            # Comparaison simple FPS vs Kuramoto
            if not check_prerequisites():
                sys.exit(1)
            
            valid, config = validate_configuration(args.config)
            if not valid:
                sys.exit(1)
            
            print("\n‚öñÔ∏è  Comparaison FPS vs Kuramoto...")
            
            fps_result = FPS_MODULES['simulate'].run_simulation(args.config, 'FPS')
            kura_result = FPS_MODULES['simulate'].run_simulation(args.config, 'Kuramoto')
            
            if MATPLOTLIB_AVAILABLE:
                fig = FPS_MODULES['visualize'].plot_fps_vs_kuramoto(fps_result, kura_result)
                output_path = f'comparison_{datetime.now():%Y%m%d_%H%M%S}.png'
                fig.savefig(output_path, dpi=150, bbox_inches='tight')
                print(f"‚úÖ Comparaison sauvegard√©e : {output_path}")
                plt.close(fig)
            else:
                print("‚úÖ Comparaison termin√©e (matplotlib manquant pour visualisation)")
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è Interruption par l'utilisateur")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå ERREUR : {e}")
        if args.verbose:
            traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()